#!/bin/bash
# Quick Start: Sistema Multi-Acelerador Optimizado

echo "ğŸš€ Sistema Multi-Acelerador Configurado"
echo ""
echo "Tu sistema tiene 3 aceleradores disponibles:"
echo ""
echo "1ï¸âƒ£  CPU (24 cores)"
echo "   â””â”€ Uso: LLMs pequeÃ±os, procesamiento general"
echo ""
echo "2ï¸âƒ£  iGPU Intel ARL (Vulkan)"
echo "   â””â”€ Uso: Ollama (LLMs 3-7B), codificaciÃ³n video"
echo ""
echo "3ï¸âƒ£  NPU Intel AI Boost 3720 (~10 TOPS)"
echo "   â””â”€ Uso: Embeddings, clasificaciÃ³n, NO LLMs"
echo ""
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo ""
echo "ğŸ“¦ Scripts disponibles:"
echo ""
echo "  ./setup-multi-accelerator.sh"
echo "    â””â”€ Optimiza CPU, verifica GPU/NPU, configura sistema"
echo ""
echo "  ./optimize-swap.sh"
echo "    â””â”€ Aumenta swap 8GBâ†’32GB y reduce swappiness"
echo ""
echo "  ./npu-embeddings-service.py demo"
echo "    â””â”€ Demo de embeddings en NPU (sentence-transformers)"
echo ""
echo "  ./benchmark-npu.py"
echo "    â””â”€ Benchmark CPU vs GPU vs NPU"
echo ""
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo ""
echo "ğŸ¦™ Ollama (iGPU) - Estado:"
docker ps --filter "name=ollama" --format "  âœ… {{.Names}} ({{.Status}})" 2>/dev/null || echo "  âš ï¸  No corriendo"
echo ""
echo "ğŸ§  NPU - Estado:"
if [ -e /dev/accel/accel0 ]; then
    echo "  âœ… Disponible: /dev/accel/accel0"
else
    echo "  âŒ No detectado"
fi
echo ""
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo ""
echo "âš¡ Quick Commands:"
echo ""
echo "  # Activar entorno OpenVINO+NPU"
echo "  source ~/.openclaw/workspace/openvino-genai-env/bin/activate"
echo ""
echo "  # Test embeddings en NPU"
echo "  python npu-embeddings-service.py demo"
echo ""
echo "  # LLM en Ollama (iGPU)"
echo "  docker exec -it ollama-intel-arc ollama run llama3.2:3b"
echo ""
echo "  # Ver uso de GPU"
echo "  intel_gpu_top"
echo ""
echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
echo ""
echo "ğŸ“Š Benchmark de tu Ãºltimo test:"
echo ""
echo "  CPU:  40.86 tokens/s  âš¡ (mejor para LLMs pequeÃ±os)"
echo "  GPU:  37.09 tokens/s  ğŸ® (actual en Ollama)"
echo "  NPU:   0.97 tokens/s  ğŸ§  (NO usar para LLMs)"
echo ""
echo "ğŸ’¡ ConclusiÃ³n: MantÃ©n Ollama en iGPU, usa NPU para embeddings"
echo ""
