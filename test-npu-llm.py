#!/usr/bin/env python3
"""
Test OpenVINO GenAI en NPU Intel
"""
import openvino_genai as ov_genai
from pathlib import Path

def test_npu_generation(model_path: str, prompt: str, device: str = "NPU"):
    """
    Ejecuta generaci√≥n de texto en el dispositivo especificado
    
    Args:
        model_path: Ruta al modelo exportado en formato OpenVINO IR
        prompt: Texto de entrada
        device: CPU, GPU, o NPU
    """
    print(f"üîß Cargando modelo desde: {model_path}")
    print(f"üéØ Dispositivo: {device}")
    
    try:
        # Crear pipeline de generaci√≥n
        pipe = ov_genai.LLMPipeline(model_path, device)
        
        print(f"\nüí¨ Prompt: {prompt}\n")
        print("ü§ñ Respuesta:")
        
        # Generar texto
        config = ov_genai.GenerationConfig()
        config.max_new_tokens = 50
        config.do_sample = False  # Greedy decoding para mayor velocidad
        
        result = pipe.generate(prompt, config)
        print(result)
        
        print("\n‚úÖ Generaci√≥n completada")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Uso: python test-npu-llm.py <model_path> [prompt] [device]")
        print("\nEjemplo:")
        print("  python test-npu-llm.py ./tinyllama-openvino 'Hello, I am' NPU")
        sys.exit(1)
    
    model_path = sys.argv[1]
    prompt = sys.argv[2] if len(sys.argv) > 2 else "Once upon a time"
    device = sys.argv[3] if len(sys.argv) > 3 else "NPU"
    
    test_npu_generation(model_path, prompt, device)
